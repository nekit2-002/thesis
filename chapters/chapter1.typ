#import "../biblio.typ": *
#import "../preamble.typ": table_figure
#show bibliography: none
// TODO this import is here only for lsp, need to remove it on release
// #bibl
// -------------------------------------------------------------------------------







= Анализ устройства и использования памяти СУБД PostgreSQL

== Анализ общего устройства PostgreSQL
Чтобы проанализировать общее устройство СУБД PostgreSQL, рассмотрим принципы взаимодействия пользователя
с системой, алгоритмы и модели, используемые системой для выдачи результатов по запросам пользователя.
PostgreSQL использует реляционную модель данных @rel, что означает, что каждый объект в ней
рассматривается как отношение и подчиняется законам реляционной алгебры. С точки зрения архитектуры
PostgreSQL основывается на клиент--серверном взаимодействии @client-server. Данный раздел посвящен
анализу архитектуры сервера Postgres, а также разбору методов работы сервера с физической памятью устройства.




=== Клиент--серверное взаимодействие и принципы ACID
Глобально система делится на две части: клиентское приложение и сервер Postgres. Приложение может с использованием
протокола взаимодействия PostgreSQL Protocol (основан на TCP/IP) подключаться к базам данных и взаимодействовать
с объектами в них. Для того чтобы не писать собственную реализацию протокола для каждого клиентского приложения,
Postgres предоставляет библиотеку libpq @libpq, порты которой на всевозможные языки программирования
обычно и являются драйверами для соединения пользовательского приложения с сервером PostgreSQL.

После успешной аутентификации и подключения к базе данных пользователь может посылать к ней запросы.
Каждый запрос выполняется в рамках транзакции (см. @fig:client), представляющей собой некоторый набор
логически связанных операций, которые мыслятся как единое целое. Каждая транзакция подчиняется подчиняется
четырем свойствам АСИД (они же ACID @acid):
+ Атомарность --- транзакция либо выполняется целиком, либо не выполняется вообще
+ Согласованность --- в начале транзакции она должна видеть согласованную целостную
  картину данных и оставить согласованную картину данных после себя
+ Изолированность --- обеспечение определенности во влиянии параллельных процессов друг на друга
+ Долговечность --- если транзакция зафиксирована, ее изменения не должны быть утеряны (даже при сбое)

#figure(
  image("../images/analysis/client-server.png", width: 70%),
  caption: [Схема транзакционного взаимодействия клиента и сервера],
) <fig:client>

Рассмотрим теперь процедуру обработки запроса (см. @fig:query). На первом этапе происходит
синтаксический анализ (проверка отсутствия ошибок в коде написанного запроса),
в результате которого получается абстрактное синтаксическое дерево (АСД) запроса, и далее
семантический анализ, выполняющий привязку узлов АСД к реальным
объектам в базе данных, информация о которых берется из системного каталога данной
БД (системный каталог --- схема БД, описывающая все подробности об ее объектах и типах их
содержимого).

После получения дерева запроса наступает второй этап, на котором дерево трансформируется
(переписывается) по некоторым правилам
(например, если в запросе содержится представление, то при переписывании оно раскрывается в
запрос, который тоже проходит этап 1 и дерево этого запроса интегрируется в дерево основного
запроса).

Когда получено окончательное дерево, включающее весь запрос, начинается этап планирования, на котором планировщик запросов перебирает различные варианты исполнения запроса,
оценивает каждый из них, и, найдя оптимальный, передает дерево плана на исполнение.

Далее запрос исполняется и результат возвращается пользователю. При исполнении запроса происходит обход дерева плана, и для каждого узла, если для него еще нет данных,
которые ему нужно получить, он спускается по узлам ниже, пока не дойдет до "листового". Когда дочерних узлов нет, происходит непосредственное получение данных из
таблицы с помощью _метода доступа_, после чего прочитанные данные передаются узлам выше, пока запрос не будет исполнен.

#figure(
  image("../images/analysis/query.png", width: 70%),
  caption: [Этапы обработки запроса],
) <fig:query>





=== Взаимодействие PostgreSQL с памятью устройства
Чтобы понять, каким образом Postgres взаимодействует с памятью устройства, на котором запущен
сервер, рассмотрим устройство работы сервера
с точки зрения процессов. Главным процессом на сервере является Postgres (он же postmaster), и
его задача состоит в том, чтобы запустить все другие процессы.
Он же создает общую память (см. @fig:memory), чтобы процессы могли обмениваться данными между собой, следит за сбоями в дочерних процессах, чтобы при
перезапускать их.

Когда появляется клиент, подключающийся к Postgres, его сначала принимает postmaster, после чего
создается обслуживающий backend процесс, и postmaster
переключает клиента на работу с этом backend--процессом. Каждый backend имеет локальную память,
в которой сохраняется вся информация, нужная конкретному клиенту, например, разобранные запросы.

#figure(
  image("../images/analysis/memory.png", width: 50%),
  caption: [Схема общего механизма обращения с памятью в PostgreSQL],
  placement: top,
) <fig:memory>

Непосредственно работа с файлами на дисках не осуществляется сервером напрямую, для этого
используется буферный кеш,
находящийся в общей памяти сервера. Буферный кеш представляет собой массив буферов, каждый из
которых хранит в себе
*страницу* данных --- единицу обмена информацией между Postgres и операционной системой, и также
дополнительную информацию (например, имя файла и положение страницы внутри этого файла).

Если какой--либо процесс собирается работать со страницей, он в первую очередь пытается найти ее
в кеше. Если страницы нет,
он обращается к операционной системе с просьбой прочитать эту
страницу и помещает ее в буферный кеш (доступ к данным при этом может оказаться достаточно
быстрым, если нужная страница найдена
в кеше операционной системы и обращения к диску не произойдет). Если в кеше оперативной памяти
нужная страница не найдена,
происходит двойная буферизация: сначала ОС копирует данные в собственный кеш, затем они попадают
в буферный кеш сервера.

Если процесс изменил данные в странице, соответствующий буфер становится "грязным". Измененная
страница подлежит записи на диск,
но по соображениям производительности запись происходит асинхронно и может откладываться. При
совместном доступе
процессов к одному и тому же месту в буферном кеше делаются блокировки для предотвращения
состояния гонки.

Размер буферного кеша обычно не так велик, чтобы база данных помещалась в него целиком, и в
таких ситуациях
имеет место алгоритм вытеснения страниц, который выбирает в кеше страницу, которая в последнее
время использовалась реже других. Если выбранный буфер оказался  грязным, страница записывается
на диск, чтобы не потерять сделанные  в ней изменения. Затем в освободившийся буфер читается
новая страница (алгоритм вытеснения LRU @lru).




== Исследование механизмов многоверсионности и очистки страниц
При при параллельной работе нескольких транзакций могут быть ситуации (например, при
одновременной записи в одну и ту же строку таблицы), когда помимо блокировки нет вариантов
разрешения состояния гонки. Тем не менее, если применять такой же подход в других случаях (в
частности, когда одна из двух транзакций пишет данные, а другая читает), возникнет проблема,
что если транзакций много, и все они будут вынуждены выстраиваться в очередь на выполнение из-за
блокировок, производительность системы сильно упадет.



=== Многоверсионность и снимки <snaps>
Чтобы оптимизировать выполнение параллельных запросов, в Postgres используется механизм
многоверсионности MVCC @mvcc1 @mvcc2, который подразумевает одновременное существование в базе
данных нескольких версий одних и тех же версий строк. При этом читающая транзакция работает со
своей версией, а пишущая --- со своей. Рассмотрим механизм работы с разными версиями строк.

#figure(
  image("../images/analysis/snapshot1.png", width: 50%),
  caption: [Схема общего механизма обращения с памятью в PostgreSQL],
) <fig:snapshot1>

На @fig:snapshot1 показана растянутая во времени конфигурация некоторой таблицы. В
системе в качестве единицы временного отсчета выступают номера транзакций $x_id$. Время
существования каждой строки определяется номерами создавшей и удалившей строку транзакций.
Обновление строки при этом воспринимается как прекращение существования старой версии строки и
начало существования новой.

Для того чтобы обеспечить согласованность данных для различных транзакций, применяются снимки
данных. Очередная транзакция, обращаясь к таблице, должна видеть только одну из имеющихся версий
каждой строки (или не видеть ни одной). Для этого транзакция работает со снимком данных,
созданным в определенный  момент времени. На @fig:snapshot1 показана ситуация, в
которой:
- У 1 строки существовала первая версия, но затем произошло зафиксированное обновление, и на
  момент снимка будет видна уже вторая версия этой строки, при этом версия существует и после
  снятия снимка, а значит она _актуальна_.
- Строка 2 на момент создания снимка уже была изменена, как и в первом случае, но после снятия
  снимка вторя версия строки сменилась на третью, которая в свою очередь была затем полностью
  удалена, поэтому строка, попавшая в снимок будет _неактуальна_.
- Строка 3 имела всего одну версию, и поскольку удалившая ее транзакция была зафиксирована,
  и произошла раньше транзакции снятия снимка, в снимке эта строка не будет видна вообще

Сам снимок представляет из себя набор чисел следующего формата:
- $x_min$ --- номер самой ранней активной транзакции на момент снимка.
- $x_max$ --- номер следующей за номером последней зафиксированной транзакции, определяющий
  момент, когда был сделан снимок
- $x i p \_ l i s t$ --- список активных на момент снятия снимка транзакций

В снимке видны изменения транзакции $x_id$, которая удовлетворяет условиям:
- $x_id < x_min$ (поскольку все изменения таких транзакций уже были зафиксированы)
- $(x_min lt.eq.slant x_id < x_max) and x_id in.not x i p \_ l i s t$
  (чтобы отобрать изменения уже завершившихся транзакций)

В зависимости от того, в какой момент времени производится снимок, транзакция будет иметь разные
уровни изоляции: для Read Commited снимок будет делаться на момент каждого запроса, а Repeatable
Read и выше --- в начале транзакции, и снимок будет один на все операции внутри транзакции.




=== Компоновка страницы и автоочистка
Для того чтобы понять, каким образом в памяти хранятся различные версии строк, рассмотрим формат
размещения данных на страницах, применяемый PostgreSQL. Согласно этому формату (см. @fig:page),
главными составляющими табличной страницы являются следующие: заголовок страницы и блок
указателей на версии строк, которые располагаются в начале страницы, и сами строки, каждая из
которых состоит из, собственно, данных строки, и заголовка строки. В середине страницы
располагается свободное пространство для вставки новых данных.

#figure(
  image("../images/analysis/page.png", width: 60%),
  caption: [Размещение данных в странице],
) <fig:page>

Размер страницы обычно составляет 8 КБ, но это значение можно изменить при сборке из исходного
кода. Заголовок страницы занимает 24 байта (с выравниванием) и содержит общую информацию о ней,
например, смещение до начала свободного пространства и специальной области, информация о размере
страницы, контрольная сумма страницы и проч.

Далее идет массив идентификаторов (указателей) строк. Каждый указатель занимает 4 байта и
содержит информацию о смещении кортежа в строке, размере этого кортежа (оба параметра в байтах),
и несколько битов, отвечающих за статус версии строки (например, normal или unused).

После свободного пространства идут данные о версиях строк. Версии строк в таблице заполняются с
конца страницы. При вставке строк всегда по возможности происходит вставка на страницу строки
целиком (если это невозможно, то строка либо сжимается, либо происходит перемещение ее части в
TOAST--таблицу). В заголовке строки биты флагов отвечают за статусы создавшей и удалившей строку
транзакций, а также за наличие строки в цепочке hot--обновлений. Также заголовок строки обычно
содержит информацию о:
- номере транзакции, создавшей строку
- номере транзакции, прекратившей существование строки
- количестве атрибутов
- битах флагов всевозможных статусов и доп. информации
- отступе до пользовательских данных
- ссылке на следующую версию строки

Описанный способ организации данных представляет собой штатный табличный
_метод доступа_ к данным в PostgreSQL и обычно называется _Heap Access Method_.

Поскольку как актуальные, так и неактуальные версии строк содержатся в одном пространстве, со
временем происходит активный расход памяти, приводящий к замедлению выполнения запросов при
обращении к таблицам. Эта проблема в Postgres решается посредством запуска процесса регулярной
очистки таблиц, называемого _vacuum_. У него есть две версии: обычная и глубокая.

Рассмотрим принцип работы обычной очистки. Для ускорения ее работы при первом запуске vacuum
создается (а при последующих --- обновляется) _битовая карта видимости_ страниц. В ней каждой
странице таблицы сопоставляется бит, говорящий о том, что в данной странице
все версии кортежей являются видимыми для всех пользователей. За счет использовании данной карты
vacuum получает возможность при повторных запусках не читать таблицу полностью, а просматривать
только те страницы, где этот бит не установлен.

Последовательность действий очистки можно описать следующим образом:
- Vacuum читает таблицу и составляет список версий строк, которые можно вычистить. Он запоминает
  идентификаторы этих строк и записывает их в части локальной памяти процесса равной значению
  параметра maintenance_work_mem.
- Пользуясь сохраненными идентификаторами, vacuum выполняет очистку индексов таблицы,
  просматривая каждый из них полностью.
- Используя те же идентификаторы, vacuum снова читает таблицу полностью, и при этом очищает ее
  от сохраненных в maintenance_work_mem строк.

К описанным действиям следует сделать несколько *замечаний*. Первое: если количество
неактуальных кортежей слишком велико чтобы поместиться в maintenance_work_mem, vacuum
циклически выполняет описанные операции, что занимает дополнительное время и аппаратные ресурсы,
потому что объекты БД вынужденно перечитываются несколько раз. Второе: *физического
перестроения* данных на уровне файлов системы *не происходит*,
а очищенные указатели просто помечаются vacuum как свободные для вставки, поэтому, если файл
объекта увеличился в процессе работы, без _полной очистки_ меньше он не станет и место не
вернется в распоряжение операционной системы.

Для того чтобы не запускать vacuum каждый раз вручную, на сервере Postgres есть постоянно
работающий фоновый процесс, называемый _autovacuum launcher_. Принцип его работы следующий: при
создании сервера в его настройках выставляется параметр _autovacuum_naptime_ (измеряется в
секундах), и autovacuum launcher раз в autovacuum_naptime секунд обращается к процессу
postmaster, чтобы тот запустил один или несколько (свой для каждой БД) рабочих процессов очистки
_autovacuum worker_, которые очищают таблицы БД.

Очищать таблицу или нет, autovacuum worker определяет по правилу:

$ N > "autovacuum_vacuum_threshold" + "autovacuum_vacuum_scale_factor" ast.op N_1 $
где:
- $N$ --- число ненужных строк в таблице
- _autovacuum_vacuum_threshold_ --- параметр, количество мертвых записей, которые должны
  быть в таблице
- _autovacuum_vacuum_scale_factor_ --- параметр, некоторый процент, часть от общего числа
  строк в таблице ($N_1$)

Тем не менее, ни vacuum, ни autovacuum worker не производят физической очистки памяти и
возвращения ее ОС. Для этой задачи используется запуск процесса командой _VACUUM FULL_
(_полная очистка_). Она полностью блокирует доступ к таблице и на запись и на чтение, поскольку
происходит физическое перестроение файлов в системе. Такая команда во время работы создает новые
файлы в табличных пространствах и копирует в них данные из старых файлов наиболее компактным
способом. Как и обычный vacuum, VACUUM FULL может занимать длительное время и требовать в
процессе работы дополнительное место на диске для новых файлов, пока старые еще существуют.

=== Связывание индексов и таблиц
Для эффективности поиска кортежей в таблице PostgreSQL использует индексы. Поскольку создание
ограничения _primary key_ на столбец таблицы создает индекс по выбранному полю, рассмотрим связь
между строками индексов и таблиц. Из @fig:page-index видно, что глобально строение
страницы одинаково как у индекса, так и у таблицы: заголовок, пространство под данные
и специальное пространство под нужды конкретного метода доступа.


#figure(
  image("../images/analysis/page-index.png", width: 70%),
  caption: [Связь индекса и таблицы],
) <fig:page-index>

// TODO втавить здесь ссылку на b-дерево
Первичный индекс основывается на B-дереве, листовые узлы которого ссылаются на на указатели
(ItemId) в начале страницы таблицы, а также содержат индексируемые поля. Такая организация,
как следствие, дает то, что при частом обновлении строк таблицы ее индексы _тоже получают
новые версии строк вместе с ней_ (то есть, если по столбцу есть индекс, и произошло обновление,
то старая версия строки появится как в таблице, так и в индексе по этому полю).

== HOT--обновления
Чтобы предотвратить избыточные обновления индексов при изменении таблицы, если индекс создан
по полю, значение которого не изменилось в результате обновления строки, можно не создавать
в B-дереве дополнительную ссылку для того же значения ключа. В этом случае PostgreSQL объединяет
версии строк в HOT(heap only tuple)--цепочки (@fig:hot).

#figure(
  image("../images/analysis/hot.png", width: 60%),
  caption: [Heap only обновление],
) <fig:hot>

При таком обновлении в индексной странице находится лишь одна ссылка --- на первую версию строки
табличной страницы, а внутри табличной страницы организуется цепочка версий, обладающая
следующими свойствами:
- строки, которые изменены и входят в цепочку, маркируются битом Heap Hot Updated
- строки цепочки, на которые нет ссылок из индекса маркируются битом HOT (то есть «только
  табличная версия строки»)
- версии строк связаны в список с помощью поля ctid, входящего в заголовок версии

Оптимизация действует только в пределах одной страницы, поэтому дополнительный обход цепочки
не требует обращения к другим страницам и не ухудшает производительность. Если на странице
не хватит свободного места, чтобы разместить новую версию строки, цепочка прервется.
На версию строки, размещенную на другой странице, придется сделать и ссылку из индекса.

Тем не менее, данная оптимизация, как и было описано выше, имеет существенный недостаток:
HOT--обновления работают только в случае, если не изменяется ни один ключ в индексах по таблице.
Иначе в каком--либо индексе появилась бы ссылка, непосредственно на новую версию строки, что
противоречит идее этой оптимизации.


== Анализ существующих решений поставленной задачи
Чтобы более отчетливо увидеть влияние autovacuum на производительность СУБД, сравним стандартный
Heap Access Method и несколько проектов, реализовавших собственные методы доступа: zheap и
orioledb. Первое расширение, ввиду давности его разработки, было встроено разработчиками
как часть ядра, в то время как второе подчиняется стандартной архитектуре расширения для
PostgreSQL (описание дается в @arch).

Оба метода доступа используют для оптимизации концепцию undo--logging @undo1 @undo2 @undo3
(более подробное объяснение алгоритма дается в @algo). При ее использовании требуется
минимальное (или не требуется вообще) задействование autovacuum, а все изменения пишутся сразу
на диск и старые версии строк не хранятся в таблицах, что приводит к тому, что они не распухают.
Использование данной методики делает процесс оперирования таблицами более устойчивым в отношении
скорости работы, поскольку даже при частых вызовах autovacuum не будет иметь работы для очистки,
и потому не забирать системные ресурсы и задерживать выполнение запросов.

Чтобы убедиться в устойчивости производительности запросов к undo-log-based таблицам, проведем эксперименты с разными настройками autovacuum, представленным в @tbl:params.

#table_figure(
  table(
    columns: 4,
    align: (left, center, center, center),
    [Номер эксперимента], [1], [2], [3],

    [autovacuum_naptime (сек)], [60], [30], [5],

    [...\_vacuum_cost_limit], [200], [600], [1200],

    [...\_insert_scale_factor], [0.2], [0.05], [0.02],
    [...\_insert_threshold], [1000], [500], [250],
    [...\_scale_factor], [0.2], [0.05], [0.02],
    [...\_vacuum_threshold], [50], [25], [10],
  ),
  "Настройки параметров autovacuum в экспериментах",
) <tbl:params>

Запрос для генерации объектов БД будет выглядеть следующим образом:
#figure(
  ```sql
  CREATE TABLE test (
      id integer primary key,
      value1 float8 not null,
      value2 float8 not null,
      value3 float8 not null,
      value4 float8 not null,
      ts timestamp not null
  ) USING ...;

  CREATE INDEX test_value1_idx ON test (value1);
  CREATE INDEX test_value2_idx ON test (value2);
  CREATE INDEX test_value3_idx ON test (value3);
  CREATE INDEX test_value4_idx ON test (value4);
  CREATE INDEX test_ts_idx ON test (ts);
  ```,
  caption: [Запрос для создания объектов БД для проверки производительности],
) <lst:creating>

Данный запрос создает таблицу с шестью полями и строит индексы по каждому из этих полей. В
зависимости от тестируемого метода доступа к таблице, она будет создаваться с суффиксом
*USING zheap* или *USING orioledb*. Запрос для бенчмарка при этом будет следующим:

#figure(
  ```sql
  \set id random(1, 10000000)
  INSERT INTO test VALUES(:id, random(), random(), random(),
  random(), now() - random() * random() * 1800 * interval '1 second')
  ON CONFLICT (id) DO UPDATE SET ts = now();
  ```,
  caption: [Запрос бенчмарка],
) <lst:benchmark>

В каждом из экспериментов бенчмарк запускается с сотней клиентов на 300 секунд в один поток. В
качестве параметров измерения производительности
выступают TPS (transactions per second) и lat (Latency).

Запрос для бенчмарка был написан таким образом, чтобы при каждом обновлении таблицы приходилось
обновлять и индекс, построенный на поле ts, и со временем, когда размеры как индексов, так и
таблицы, стали бы больше, скорость вставки в них стала бы стремительно проседать, за счет
накопления большого объема мертвых строк из--за интенсивных обновлений и большого количества
параллельных сеансов.

Рассмотрим влияние такого бенчмарка на стандартный метод доступа в PostgreSQL (@vanilla). При 
стандартных настройках autovacuum а задержке (Latency) имеются пики значений, самое большое из 
которых достигает 50 миллисекунд. Тем не менее, смотря на график в целом, можно увидеть, что 
подавляющее большинство этих пиков едва превышает значения в 26 миллисекунд, а самый большой 
является единственным выбросом. Что касается TPS,
его поведение можно разделить на 4 периода: 0 --- 50 сек., 50 --- 120 сек. и 120 -- 180 сек. и 
180 --- 300 сек. В первом периоде происходит
активная вставка данных, но поскольку таблицы быстро распухают, TPS начинает стремительно 
снижаться. Во втором периоде
с приходом вакуума нагрузка на систему увеличивается и происходят резкие флуктуации TPS, который 
пока что держится на уровне от 10 000
в худшем случае до 30 000 на пиках активности системы. На третьем периоде, когда вакуум закончил работу, резкость флуктуаций задержки и ТPS
значительно снижается, но из--за большого объема уже накопившихся актуальных данных TPS оказывается в диапазоне от 7 000 до 20 000. На последнем
этапе, когда вакуум снова приходит очищать таблицу нагрузка опять усиливается, задержка увеличивается (отсюда повышение количества пиков синего графика).
TPS при этом снижается окончательно, и уже лежит в границах от 1000 до 10 000.

На следующем эксперименте, когда autovacuum работает чуть более агрессивно, большой пик с предыдущего опыта сохраняется, на первом периоде поведение системы такое же,
как и в предыдущем случае. Во втором периоде во время резких флуктуаций и непосредственно после них, можно увидеть, что общий уровень задержек в ответе на запросы поднялся,
достаточно стабильно лежит выше 10, тогда как в первом случае большая часть значений задержки лежала ниже этого значения. Аналогичная картина видна относительно третьего
этапа, на котором большая часть задержек по времени близка к 20 миллисекундам, в то время как в первом эксперименте в это время она была в районе 10.
Общий уровень TPS меняется несильно, но слегка смещается вниз относительно уровня в 10 000. В заключительном периоде наблюдается
значительное увеличение амплитуды задержки, значение которой теперь регулярно достигает уровня 30 мс и больше. Это говорит о том, что
более агрессивная работа очистки увеличила нагрузку на систему.

В заключительном эксперименте первый период не отличается от остальных (потому что пока что не накопилось работы для очистки и autovacuum даже при запуске
не выполняет работу долго). Во втором периоде флуктуации в TPS становятся еще чаще и резче, чем в двух первых экспериментах,
а пиковая задержка в моменте достигает 200 миллисекунд. На третьем этапе задержка теперь стабильно выше 20мс а общий уровень TPS лежит по большей части
ниже значения в 10 000. Заключительный период теперь намного более загруженный, чем в эксперименте 2 (это можно увидеть, поскольку пики задержек и просадки TPS
начинаются в самом начале периода, а не ближе к середине, как это было во 2 эксперименте). Относительно обоих предыдущих экспериментов можно увидеть,
что количество пиков задержек увеличилось, а по амплитуде они стали в разы превышать те, что были в первых двух экспериментах.

#figure(
  image("../images/analysis/vanilla.png"),
  caption: [Эксперименты с autovacuum на стандартном Heap Access Method],
  placement: top,
) <vanilla>


Из описанного выше анализа, можно сделать вывод, что в случае обычного Heap Access Method _скорость системы стремительно снижается с увеличением
активности autovacuum_.

Далее рассмотрим результаты экспериментов, проводимых с теми же условиями и скриптами, но на zheap (@zheap) и orioledb (@oriole).
Для zheap в первом периоде первого эксперимента просадок TPS немного больше, поскольку операция вставки в нем более комплексная по своей сути, чем
у простого Heap Access Method, хотя его общий уровень, как и в случае стандартного метода доступа держится в пределах от 10 до 30 тысяч транзакций
во время активной работы системы. Задержки также лежат ниже уровня в 10 миллисекунд.
На втором этапе количество флуктуаций TPS заметно меньше, чем у ванильной (стандартной) версии метода доступа, а его общий уровень падает ниже 20 000
заметно позднее. Задержки при этом стабильно находятся на уровне ниже 20 мс. Аналогичная картина с уменьшением количества флуктуаций заметна и на
третьем периоде. На четвертом периоде количество просадок TPS и роста задержек уже увеличивается, что связано с увеличением количества актуальных данных в
таблице и ее индексах. Амплитуды задержек при этом больше, чем у стандартной версии.


#figure(image("../images/analysis/zheap.png"), caption: [Эксперименты с autovacuum на zheap]) <zheap>

При втором и третьем экспериментах с хранилищем zheap на всех периодах картина примерно такая же, как и при первом. Количество пиков задержек
и просадок по TPS примерно одинаковое, с тем лишь исключением, что некоторые пики задержек упали с 60 до 40 мс. Также следует отметить, что общее снижение TPS на уровень ниже 10 000 при активном использовании
autovacuum у zheap наступает значительно позже: на втором эксперименте на третьем периоде у стандартного хранилища TPS колеблется то ниже то выше 10 000,
в то время как zheap он стабильно и значительно выше этого значения; на третьем эксперименте, начиная примерно со 130-ой секунды, у стандартного хранилища
TPS почти всегда находится ниже 10 000, в то время как у zheap это падение происходит только на 170-ой секунде.
Все это говорит о том, что в сравнении со стандартным хранилищем zheap _намного менее подвержен влиянию
autovacuum на производительность запросов_.

Рассмотрим теперь результаты результаты экспериментов, проведенных с хранилищем orioledb (@oriole). В силу того, что проект разрабатывается
и улучшается в настоящий момент, в нем использованы более современные решения в написании кода, поэтому в сравнении и со стандартным Heap Access Method,
и с zheap, в первом периоде каждого из экспериментов (0 --- 50 сек) просадок по TPS, а также резкого возрастания задержек не наблюдается. Во
втором периоде падение TPS хотя и произошло, но резких флуктуаций ни в задержках, ни в TPS так же не наблюдается во всех трех экспериментах.
Правая граница третьего этапа для данного хранилища несколько смещена влево относительно предыдущих и составляет 160 секунд. До этого времени
во всех экспериментах общий уровень TPS стабильно находится выше границы в 10 000, хотя при слишком большой активности вакуума (3 эксперимент) на 150-ой
секунде наблюдается спад TPS и рост задержки, что говорит о том, что избавиться от его влияния полностью не получилось. Что касается
пиков задержек, то их общее количество после 160-ой секунды одинаково. Амплитуды разные, но в целом картина активностей и спадов
в работе системы говорит о том, что они происходят в одно и то же время во всех трех экспериментах, а значит изменяющаяся амплитуда является следствием не работы
вакуума, а каких-то других процессов. как и в случае с zheap, общая картина графиков для трех экспериментов одинакова, а значит orioledb тоже
_мало подвержен воздействию активно работающего autovacuum_.


#figure(image("../images/analysis/oriole.png"), caption: [Эксперименты с autovacuum на orioledb]) <oriole>

== Выводы
В результате анализа была изучена общая архитектура системы управления базами данных PostgreSQL.
Также были изучены принципы ее взаимодействия с памятью и механизм многоверсионности. Были
проанализированы способы очистки таблиц и памяти средствами Postgres. Был проведен сравнительный
анализ производительности различных
хранилищ, предоставляющих альтернативные методы доступа к таблицам. В результате анализа было
выявлено, что использование хранилищ, основанных на undo--logging, повышает общую
производительность СУБД, а также дает устойчивость при увеличении нагрузки на систему за счет
агрессивного использования процесса очистки.


#pagebreak()
